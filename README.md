# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
- This dataset contains information related to a direct marketing campaign of a Portuguese banking institution and if their clients did subscribe for a term deposit.<br>
We seek to predict if clients are subscibed for a term deposit based on the provided personal and financial information.

- The best performing model is a SoftVotingClassifier (VotingEnsemble), containing MaxAbcScaler and XGBoostClassifier which implements the Gradient tree boosting algorithm_  well known for its efficiency to predict accuracies. 
This model has been chosen among several models based on the highest accuracy, while tuning the hyperdrive parameters during the experiment.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Below a general pipeline architecture is provided to explain the workflow of using Azure SDK with HyperDrive parameterization:

![alt_text](SklearnPipelineArchitecture.PNG)

Getting good or bad results is related to how well prepared your model is? <br>
The first part of the process will be to prepare the model for your experiment:<br>
1 - **The dataset and the cleaning:** We used a Cleaning function included in the _train.py_ Script. <br>
The main performed actions inside the script are: <br>
   - Retrieve data from the specified path using TabularDatasetFactory method.<br>
   - Convert the dataset to a binary representation, by applying the _SKLearn get_dummies()_ method , to use later with <br>
   - Split data into training and testing sets, using Sklearn function  *train_test_split* .<br>
   - Apply the SKLearn _LogisticRegression_ classifier by specifying the sample parameters, then fit the split data.<br>
   - Finally verify the accuracy and register the model.<br>
The problem with handling predictions manually is the time lost while trying to tune the parameters seeking for the best result.<br>
Hence, the benefit of Azure HyperDrive is finding the perfect fit by tuning the hyperdrive parameters among a pre-specified random set of your choice.
To prepare the HyperDrive configuration, we need to set three major parameters including:<br>
   1- Specify a parameter sampler: since we are using the SKLearn _LogisticRegression_ classifier we will be using:<br>
      - The inverse of regularization strength _**C**_ with a default value of _1.0_, you need to specify a descrite set of options to sample from.<br>
      - And, the maximum number of iterations taken for the solvers to converge _**max_iter**_ <br>
   2- Specify an early termination policy: Among three types, we decided to work with the _Bandit Policy_ which is classified as an agressive saving, as it will terminate any         job based on a _slack_ criteria, and a _frequency_ and _delay_ interval for evaluation. <br>
      - slack_factor: The slack is specified as a ratio used to calculate the allowed distance from the best performing experiment run.<br>
      - evaluation_interval: Reflects the frequency for applying the policy.<br>
      - delay_evaluation: Reflects the number of intervals for which to delay the first policy evaluation.<br>
   3 - Create a SKLearn estimator to use later within the HyperDriveConfig definition.<br>
   The estimator contains the _source directory_ _ The path to the script directory _ , the _compute target_ and the _entry script_ _ The name of the script to use along the experiment _.<br>
2- **The algorithm choice:** Choosing an algorithm for a model must base on the metrics you need to predict: Continuous or Discrete?<br>
Since we want to estimate if the client did or not subscribe to a deposit term account, we applied the _SKLearn LogistigRegression_ classifier taking as parameters: the cleaned then split dataset into a training/testing dataset using SKLearn function *train_test_split* 
 

**What are the benefits of the parameter sampler you chose?**

**What are the benefits of the early stopping policy you chose?**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
